{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "$X = (\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_n)$ is a collection of vectors encoding some information.\n",
    "\n",
    "We want to find a direction in space along which data vary the most.\n",
    "\n",
    "Firstly, center the data:\n",
    "\n",
    "$$\\overline{\\mathbf{x}}_i = \\mathbf{x}_i - \\frac{1}{n}\\sum_{k=1}^n{\\mathbf{x}_k}$$\n",
    "\n",
    "Given a direction $\\mathbf{v}$ (i.e. unit vector) the projection of $\\overline{\\mathbf{x}}_i$ on it will be $\\overline{\\mathbf{x}}_i^T \\mathbf{v}$.\n",
    "\n",
    "We can project all centered points with one matrix multiplication: $\\mathbf{p} = \\overline{X}^T \\mathbf{v}$.\n",
    "\n",
    "$\\mathbf{p}$ is the resulting vector of projections.\n",
    "\n",
    "Our goal is to find unit $\\mathbf{v}$ which maximizes $Var[\\mathbf{p}] = \\frac{1}{n}\\sum_{i=0}^n{(p_i - E[\\mathbf{p}])^2}$.\n",
    "\n",
    "Since data is centered, $E[\\mathbf{p}] = 0$. Hence \n",
    "\n",
    "$$Var[\\mathbf{p}] = \\frac{1}{n}\\sum_{i=0}^n{p_i^2} = \\frac{1}{n}\\mathbf{p}^T \\mathbf{p} = \\frac{1}{n} \\mathbf{v}^T \\overline{X} \\overline{X}^T \\mathbf{v}$$\n",
    "\n",
    "We already know how to maximize this expression! The answer is the eigenvector of $\\frac{1}{n} \\overline{X} \\overline{X}^T$ which corresponds to the greatest eigenvalue. Moreover, if $\\mathbf{v}$ is an eigenvector:\n",
    "\n",
    "$$Var[\\mathbf{p}] = \\mathbf{v}^T \\frac{1}{n} \\overline{X} \\overline{X}^T \\mathbf{v} = \\lambda \\mathbf{v}^T \\mathbf{v} = \\lambda$$\n",
    "\n",
    "\n",
    "# Toy example: Artificial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "# pca takes centered data matrix\n",
    "def pca(data):\n",
    "    covariance_matrix = (data.transpose() @ data) / data.shape[0]\n",
    "    \n",
    "    eigvals, eigvecs = np.linalg.eig(covariance_matrix)\n",
    "    \n",
    "    sorted_idx = np.argsort(eigvals)[::-1]\n",
    "    \n",
    "    return eigvals[sorted_idx], eigvecs[:,sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points\n",
    "N = 100\n",
    "\n",
    "# Data dimension\n",
    "D = 3\n",
    "\n",
    "# Generate data\n",
    "data = np.random.normal(size=(N, D))\n",
    "center = np.mean(data, axis=0)\n",
    "data = data - center\n",
    "\n",
    "eigvals, eigvecs = pca(data)\n",
    "# Eigenvalues are real and positive\n",
    "print(eigvals)\n",
    "\n",
    "# Eigenvectors are orthogonal\n",
    "products = eigvecs.transpose() @ eigvecs\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "\n",
    "ax.scatter(data[:, 0], data[:, 1], data[:, 2])\n",
    "for v in np.transpose(eigvecs):\n",
    "    ax.plot3D([0, v[0]*3], [0, v[1]*3], [0, v[2]*3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points\n",
    "N = 100\n",
    "\n",
    "# Random Embedding Matrix\n",
    "embedding = np.random.normal(size=(2, 3))\n",
    "\n",
    "# Generate and center data\n",
    "data = np.random.normal(size=(N, 2)) @ embedding + np.random.normal(size=(N, 3))*0.1\n",
    "center = np.mean(data, axis=0)\n",
    "data = data - center\n",
    "\n",
    "eigvals, eigvecs = pca(data)\n",
    "print(f'Eigenvalues are {eigvals}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "\n",
    "ax.scatter(data[:, 0], data[:, 1], data[:, 2])\n",
    "for v in np.transpose(eigvecs):\n",
    "    ax.plot3D([0, v[0]*3], [0, v[1]*3], [0, v[2]*3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data into eigenvectors coordinate system\n",
    "transformed_data = data @ eigvecs\n",
    "print(f'Variance along first eigenvector: {np.var(transformed_data[:, 0])}')\n",
    "print(f'Variance along second eigenvector: {np.var(transformed_data[:, 1])}')\n",
    "print(f'Veriance along third eigenvector: {np.var(transformed_data[:, 2])}')\n",
    "\n",
    "print(f'Eigenvalues: {eigvals}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neglect the dimension which corresponds to the lowest eigenvalue\n",
    "transformed_data[:, 2] = 0.\n",
    "\n",
    "# Transform to the original coordinate system\n",
    "transformed_back = transformed_data @ eigvecs.transpose()\n",
    "\n",
    "average_squared_distance = np.sum((data - transformed_back)**2) / N\n",
    "print(f'Average Squared Distance from original data = {average_squared_distance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EigenFaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('face_data.csv')\n",
    "\n",
    "# 400 Samples, 64x64 images in shades of gray + 1 target label = 4097\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_faces(pixels):\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(6, 6))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(np.array(pixels)[i].reshape(64, 64), cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "plot_faces(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring the data to the format for our pca function\n",
    "data = np.array(X)\n",
    "\n",
    "# Center the data\n",
    "center = np.mean(data, axis=0)\n",
    "\n",
    "centered_data = data - center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA.\n",
    "eigvals, eigvecs = pca(centered_data)\n",
    "eigvals, eigvecs = np.real(eigvals), np.real(eigvecs)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(eigvals[:50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot eigenfaces\n",
    "eigenfaces = (eigvecs + center).transpose()\n",
    "\n",
    "plot_faces(eigenfaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to first 25 eigenvectors\n",
    "transformed_data = centered_data @ eigvecs[:, :25]\n",
    "\n",
    "print(f'Shape of the transformed data is {transformed_data.shape}')\n",
    "\n",
    "transformed_back = transformed_data @ eigvecs[:, :25].transpose() + center\n",
    "\n",
    "plot_faces(transformed_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread('cat.png')\n",
    "\n",
    "print(img.shape) \n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat every column as a vector\n",
    "data = np.reshape(img, (900, 900*4)) \n",
    "print(data.shape) \n",
    "\n",
    "# Center the data\n",
    "center = np.mean(data, axis=0)\n",
    "\n",
    "centered_data = data - center\n",
    "\n",
    "# Compute PCA.\n",
    "eigvals, eigvecs = pca(centered_data)\n",
    "eigvals, eigvecs = np.real(eigvals), np.real(eigvecs)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(eigvals[:50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to first 30 eigenvectors\n",
    "transformed_data = centered_data @ eigvecs[:, :30]\n",
    "\n",
    "# shape is (50, 900). Compression rate is 72!\n",
    "print(f'Shape of the transformed data is {transformed_data.shape}')\n",
    "\n",
    "transformed_back = transformed_data @ eigvecs[:, :30].transpose() + center\n",
    "\n",
    "# reshape the transformed back data\n",
    "rec_img = np.reshape(transformed_back, (900, 900, 4))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(rec_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gram Matrix \n",
    "\n",
    "$X = (\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_n)$ is a collection of vectors encoding some information.\n",
    "\n",
    "Let $\\mathbf{v}$ be an eigenvector of covariance matrix. That is\n",
    "\n",
    "$$\\frac{1}{n} \\overline{X} \\overline{X}^T \\mathbf{v} = \\lambda \\mathbf{v}$$\n",
    "\n",
    "Multiply by $\\overline{X}^T$ from the left:\n",
    "\n",
    "$$\\frac{1}{n} \\overline{X}^T \\overline{X} \\overline{X}^T \\mathbf{v} = \\lambda \\overline{X}^T \\mathbf{v}$$\n",
    "\n",
    "The matrix of pair-wise scalar products $\\overline{X}^T \\overline{X}$ is called *Gram Matrix*. Denote it $K$:\n",
    "\n",
    "$$ K \\overline{X}^T \\mathbf{v} = n \\lambda \\overline{X}^T \\mathbf{v}$$\n",
    "\n",
    "$\\overline{X}^T \\mathbf{v}$ is an eigenvector for that matrix! Remember it was a vector of projections of data points on the eigenvector. However numerical computation will give us this vector normilized. To get the projection vector we need to multiply it by the norm of $\\overline{X}^T \\mathbf{v}$:\n",
    "\n",
    "$$ \\lVert \\overline{X}^T \\mathbf{v} \\rVert = \\sqrt{\\mathbf{v}^T \\overline{X} \\overline{X}^T \\mathbf{v}} = \\sqrt{n \\lambda} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca takes centered data matrix\n",
    "def pca2(data):\n",
    "    gram_matrix = (data @ data.transpose()) / data.shape[0]\n",
    "    \n",
    "    eigvals, eigvecs = np.linalg.eig(gram_matrix)\n",
    "    \n",
    "    sorted_idx = np.argsort(eigvals)[::-1]\n",
    "    \n",
    "    return eigvals[sorted_idx], eigvecs[:,sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points\n",
    "N = 10\n",
    "\n",
    "# Random Embedding Matrix\n",
    "embedding = np.random.normal(size=(2, 3))\n",
    "\n",
    "# Generate data\n",
    "data = np.random.normal(size=(N, 2)) @ embedding + np.random.normal(size=(N, 3))*0.1\n",
    "\n",
    "eigvals, eigvecs = pca(data)\n",
    "print(f'Eigenvalues of covariance matrix are {eigvals}')\n",
    "\n",
    "eigvals2, eigvecs2 = pca2(data)\n",
    "eigvals2, eigvecs2 = np.real(eigvals2), np.real(eigvecs2)\n",
    "print(f'First 3 eigenvalues of gram matrix are {eigvals2[:3]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection1 = eigvecs2[:, 1] * np.sqrt(eigvals2[1] * data.shape[0])\n",
    "projection2 = data @ eigvecs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"projection from eigendecomposition of covariance matrix: \\n{projection1}\")\n",
    "print(f\"projection from eigendecomposition of gram matrix: \\n{projection2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.load_iris()\n",
    "\n",
    "print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
